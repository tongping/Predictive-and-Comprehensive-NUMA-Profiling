\section{Implementation}
\NP{} used instrumentation in instruction level to insert a handle function call for each Load and Store instructions.And by the interception, \NP{} can simulate how the memory is accessed in both page and cache level, and collect the sharing pattern we need when the memory object is freed by users.If they are never freed explicitly, we can collect the sharing information in the end of the whole application.Besides, we also intercepted lock function and collected thread based information to help detect thread migration problems and load imbalance problems.
\subsection{Compiler-Based Instrumentation} 
The instrumentation is achieved by LLVM in the \NP{}. Basicly, LLVM will compile programs into IR (intermediate representation) which is also called LLVM Assembly Code, then it provides lots of libraries and tools to manipulate the IR, like insert,delete or replace assembly codes or function calls.
To intercept the Load and Store instructions, we made a pass function which is loaded by LLVM and it will traverse each files,functions to identify target points and insert a function call. The function all is implemented by \NP{}, through which \NP{} could got each memory access operation from target applications with its address and access type(read or write).

It is worth to notice \NP{} only focused on memory access to heap objects, since heap objects have much higher chance to be shared for multiple threads, which leads potential NUMA issues. If a object is only accessed by one single thread, it will never have NUMA issues, except threads migrated to another nodes and \NP{} could also detect this problem that is detailed described in the following parts.Besides, the pass function should be placed at the very end of compiling optimization phase of LLVM,since most memory access instructions will be optimized out after huge amounts of compiling optimization strategies applied by LLVM. Based on our experiments, the number of memory access instructions could be reduce by half or more.So if the order of the pass function is not set properly, huge amounts of duplicated instrumentation calls will be made which not only brings more overheads but also makes the profiling results not accurate.

What are the challenges of that? 
What type of issues that we usually have. I remembered that we used to have high overhead with compiler-based instrumentation.

What particular issue for the version of instrumentation. 

\subsection{Detecting Remote Accesses Caused by Sharing}
In the beginning, there are some assumptions \NP{} made here that need to be clarified first.To make \NP{} independent from any hardware, OS and scheduling algorithm, \NP{} assumed that each thread could be scheduled to any node randomly. So we considered each thread running on a single virtual node with independent cache lines, and all the sharing information is traced based on thread level, which means if thread-A tries to access a memory page which is first touched by thread-B, this access will be considered as a remote access without considering which node thread-A and thread=B are located on in the real machine.Because of the first touch policy applied by OS, the memory page will be located on the node that the first touching thread located on.In the \NP{}, we also record the first touching thread id for each virtual page.

In the \NP{}, we simulated memory access in both page level and cache level. They could contain different kinds of problems, and each one could be fixed by a different strategy, so by differentiate them \NP{} could give user more accurate help information like which strategy is more suitable in each case. Further more, by trace cache invalidation, \NP{} could reveal more potential NUMA issues compared with other existing works, since memory access occurs only after a cache miss happened, and cache level contention could increase cache invalidation and cache miss rate super significantly.

In the beginning, \NP{} just simply traced how many writing operations happened in each cache line, and if the writing number exceeds a threshold, \NP{} will start to trace more detailed information. According to this mechanism, only the cache line with high writing volume will be carefully traced, which could help to reduce massive both memory and performance overheads.Besides this will not decade the effects of \NP{}, since in this level we only focused on the cache invalidation caused by writing operation and cache capacity invalidation is mainly counted in the page level.So in detail, if a reading operation happened, \NP{} will record that this piece of memory will be copied into the cache line of the demanding thread.And when a writing operation happened, all the copies in each virtual node will be invalidated.We can imagine that if the memory is extensively shared among threads and also contain huge amounts of writing operation,the invalidation number could be super huge, which further more brings huge amounts of NUMA remote accesses. 

Besides, to provide more help information to users about what is root cause of the problems and also to fix them, \NP{} also detect and differentiated false sharing problems and true sharing problems.To achieve this goal, \NP{} traced writing operation in word level, if a cache line is extensively updated by multiple threads.For each word, if it is updated by multiple threads, \NP{} will mark it as true sharing, unless, false sharing will be marked.This information could help user to determine the root cause and also what strategies could be applied.Further more, \NP{} also traced how many continuous reading operations contained in each cache line.If a cache line contains massive continuous reading operations,users could optimize this piece of memory by duplicating it over NUMA nodes, so that the continuous reading operations will become local access, and the overheads used to make them consistent will be very small.

In the page level, it is very similar with what \NP{} did in the cache level.First of all, \NP{} only record how many memory access happened in each virtual page, and if it exceeds a threshold, a more detailed information will be recorded.In the detailed information, \NP{} will record how many remote access happened from every thread, and to provide more detailed access pattern information, this information is collected in cache line wised inside a virtual page.In this way, users will have a very clear view about the access pattern which could give some clues about how to eliminate this issue.For example, if there is no explicit regular access pattern, as far as we know the best way is to make it interleaved over nodes.On the other head, if each thread mainly access a certain part area of pages, users can also consider about memory partition.

\NP{} can not avoid big overheads due to the use of instrumentation.Especially for an application that contains very serious NUMA issues or even false sharing issues, since the issues will be doubled in the \NP{}.To eliminate this problem, \NP{} also did some sampling inside to reduce overheads, but without influencing its effects.Specifically, \NP{} make sampling for all read operations but not for writing operation.The reason is that the volume of reading is much bigger than writing, which means reading operation could bring much more overheads inside \NP{}.Besides, the sampling only reduced the granularity but will contain very tiny influence on the security.

\todo{write a part about how to compute the metric}

%False sharing/ true sharing. 
%Duplicatable detect.
%Cache Invalidation Number (instead of using latency) 

\subsection{Detecting Remote Accesses Caused by Potential Migration} 
Even developers made a lot of efforts to do memory distribution or solve cache line sharing problems, they could still find not too much improvements in performance.One of very crucial reasons is that threads migrated to another node, but memory stayed still. So no matter how big efforts developers made to optimize their code, remote access still can not be eliminated.Even worse, in most cases stack memories are only accessed by a single thread, which are initially located on the same node with that thread.But the local access will become remote access after the thread makes migration.That is one reason that applications can get huge performance improvements just by simply thread binding based on out extensive experiments.

Typically, there is a big chance that threads made migration if lock contention happened.Unless it is highly possible the thread will be rescheduled to the origin core.So \NP{} will only check whether a thread migrates to another node or not when the thread failed to request a block lock, and this also helps to reduce the overheads caused by frequently kernel function call to get current NUMA node id.To check whether a thread failed to acquire a blocking lock, \NP{} also intercept all blocking lock function calls in standard C library, like pthread\_mutex\_lock and pthread\_barrier\_wait.Inside the \NP{}, it stimulated the behaviors of each blocking lock before acquired the real lock function,so that we could get the lock contention status before the thread goes blocked.

\subsection{Detecting Load Imbalance}
We also propose two mechanisms to solve load imbalance issues according memory access pattern.


%We are based on the number of memory accesses of every thread. 
