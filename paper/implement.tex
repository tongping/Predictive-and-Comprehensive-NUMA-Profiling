\section{Design and Implementation}
\label{sec:implementation}

This section elaborates \NP{}-Static and \NP{}-Dynamic. \NP{} leverages compiler-based instrumentation (\NP{}-Static) to insert a function call before memory access, which allows \NP{}-Dynamic to collect memory accesses. \NP{} utilizes a preloading mechanism to intercept synchronizations and memory allocations, without the need of changing programs explicitly. Detailed design and implementation are discussed as follows. 

\subsection{\NP{}-Static} 
\NP{}'s static component (\NP{}-Static) performs the instrumentation on memory accesses. In particular, it utilizes static analysis to identify memory accesses on heap and global variables, while omitting memory accesses on local variables. Based on our understanding, local variables typically will never cause performance issues. \NP{}-Static inserts a function call 
upon these memory accesses, where this function is implemented in \NP{}-Dynamic library. In particular, this function provides detailed information on the access, including the address, the type  (i.e., read or write), and the number of bytes.  

\NP{} employs the LLVM compiler to perform the instrumentation~\cite{llvm}. It chooses the intermediate representation (IR) level for the instrumentation due to the flexibility, same as AddressSanitizer~\cite{AddressSanitizer}, since LLVM provides lots of APIs and tools to manipulate the IR. The instrumentation pass is placed at the end of the LLVM optimization passes, where only memory accesses surviving all previous optimization passes will be instrumented.  \NP{}-Static traverses all functions one by one, and instruments a function call before memory accesses on global and heap variables. The instrumentation is adapted from AddressSanitizer~\cite{AddressSanitizer}.


\subsection{\NP{}-Dynamic}

This subsection starts with tracking application information, such as memory accesses, synchronizations, and memory allocations. Then it discusses the detection of each particular performance issue.  In the following, \NP{} is used to represent \NP{}-Dynamic unless noted otherwise. 

\subsubsection{Tracking Accesses, Synchronizations, and Memory Allocations}

\NP{}-Dynamic implements the inserted function before memory accesses, allowing it to track memory accesses. Once a memory access is intercepted, \NP{} performs the detection as discussed below. 

\NP{} utilizes a preloading mechanism to intercept synchronizations and memory allocations before invoking correspond functions.
\NP{} intercepts synchronizations in order to detect possible thread migrations, which will be explained later. \NP{} also intercepts memory allocations so that we could attribute performance issues to different callsites, assisting data-centric analysis~\cite{XuNuma}. For each memory allocation, \NP{} records the allocation callsite and its address range. 
\NP{} also intercepts thread creations in order to set up per-thread data structure. 
%In particular, it assigns each thread a thread index.


\subsubsection{Detecting Normal Remote Accesses}
\label{sec: remote_access}


\NP{} detects a remote access if the current thread accesses a page that was accessed by a different thread for the first time. This is based on the assumption that the OS typically allocates a physical page from the node of the first accessor due to the default first-touch policy~\cite{firsttouch}.  Similar to existing work, \NP{} may over-estimate the number of remote accesses, since an access is not a remote one if the data is still in the cache line. 
However, this shortcoming can be overcome easily by only reporting issues larger than a specified threshold, as exemplified in our evaluation (Section~\ref{sec:evaluation}).  

\NP{} is further designed to reduce its performance and memory overhead when tracking a page's initial accessor. A naive design is to employ hash table for tracking such information. Due to a large number of memory accesses, this design (e.g. hash table) could easily introduce significant performance overhead since multiple pages are mapped to the same bucket. Instead, \NP{} maps a continuous range of memory to track such information with the shadow memory technique~\cite{qinzhao}. The use of shadow memory only requires a simple computation to locate the data, which also avoids the conflicts of using hash table. 
\NP{} also maintains the number of accesses for each page in the same map. We observed that a page without a large number of memory accesses will not cause a significant performance slowdown. Based on this, \NP{} only tracks the detailed accesses for a page when the number of accesses on it is larger than a pre-defined (configurable) threshold. 
Since the recording uses the same data structures, \NP{} designs a slab-like allocator that keeps a pool of objects, without resorting to the default allocator.  

For each page with the number of accesses larger than a threshold, \NP{} tracks the following information. \textit{First}, it tracks the threads accessing this page, which helps determine the fix strategy. For instance, if only two threads access this page, then we could bind these two threads to the same physical node for the better performance. The detailed fixing strategy is out of the scope, but Section~\ref{sec:casestudies} provides multiple examples. 
\textit{Second}, \NP{} further tracks the number of accesses on each cache line.  
%records the corresponding access number. Such information will be helpful for data-centric attribution, since an object may only cover part of some pages. 
This enables us to compute the number of remote accesses of each object more accurately when an object is smaller than a page. \textit{Third}, \NP{} further checks whether an object is exclusively read after the first write or not. For such cases, it is good to duplicate the data to multiple nodes, helping improve the locality. 
\textit{Last not least}, \NP{} maintains  word-level information for cache lines with excessive cache invalidations, as further described in Section~\ref{sec: cacheline}.

\textbf{Remote (Access) Score:} \NP{}  proposes a performance metric -- remote score -- to evaluate the seriousness of remote accesses. An object's remote score is defined as the number of remote accesses within a specific interval, which is currently set as one millisecond. Typically, a higher score indicates a more serious performance issue, as shown in Table~\ref{tab:numa_issues}. 
For pages with both remote accesses and cache invalidations, we will check whether cache invalidation is dominant or not. If the number of cache invalidations is larger than 50\% of remote accesses, then the major performance issue of this page is caused by cache invalidations (i.e. false or true sharing). We will omit remote accesses instead. 
 
\subsubsection{Detecting False and True Sharing Issues}
\label{sec: cacheline}

Based on our observation, cache coherence has a higher performance impact than normal remote accesses. Cache coherence problems can be further caused by false and true sharing, where multiple threads are accessing the same cache line simultaneously. Differently, false sharing occurs when multiple threads are accessing different words, while true sharing is caused by accesses to the same words. \NP{} separates false and true sharing from remote accesses, which is different from all NUMA profilers, since they may require a different fixing strategy. For instance, false sharing can be fixed by the padding,  where remote accesses typically are fixed with interleaved page allocations.

\NP{} detects false/true sharing with a similar mechanism as Predator~\cite{Predator}, but adapting it for the NUMA architecture. Predator tracks cache invalidations as follows: if a thread writes a cache line that has been accessed by a different thread recently, this write operation introduces a cache invalidation. However, this mechanism underestimates the number of cache invalidations. Whenever multiple cores hold the same cache line, this  write operation should introduce more than one cache invalidation. 
Therefore, \NP{} tracks the number of threads that have loaded the same cache line and increases the number of cache invalidations correspondingly. Further, \NP{} differentiates remote cache invalidations from local ones: if a thread should load the memory from a remote node, then it is a remote cache invalidation. Otherwise, it is a local cache invalidation. \NP{} only focuses on remote cache invalidations that have a higher impact on the NUMA machine.  


\textbf{False/True Sharing Score:} 
\NP{} further proposes false/true sharing scores for each corresponding object, which is lacked in Predator~\cite{Predator}. The scores are computed by dividing the number of cache invalidations with the product of time (milliseconds) and the number of threads. The number of threads is employed to reduce the impact with different parallelization degree. That is, \NP{} could report the similar scores when running on a machine with different number of cores (and threads).
\NP{}  differentiates false sharing from true sharing by recording word-level information.

\subsubsection{Detecting Issues Caused by Thread Migration}

As discussed in Section~\ref{sec:intro}, \NP{} identifies applications with excessive thread migrations, which are omitted by all existing NUMA profilers. Thread migration may introduce excessive remote accesses because of the following reasons: (1) after the migration, a thread is forced to reload all data from the original node, and access its stack remotely afterwards; (2) All deallocations from this migrated thread may be returned to freelists of remote nodes, causing more remote accesses afterwards, which is based on our understanding of most allocators.    

%\todo{Stop checking start}

\textbf{Thread Migration Score (TMS):} \NP{} evaluates the seriousness of thread migrations with the thread migration score. This score is computed as the following formula: 
%\todo{Jin: Add the formula}
\begin{equation}
%S = P \cdot (\underset{i \in T}{\sum } c_{t} / (R \cdot |T|))
TMS =  \dfrac{\sum_{i=0}^{|T|} {M_i}}{R} \cdot P \label{equation:tms}
\end{equation}

%\todo{why would the total number of threads looks like the absolute value of the number of threads? : I think T represents a collection, |T| means the size of the collection.}
%\todo{What this formula means? What do you mean R * |T|? Why we design like this? Do we only care about the parallization and assume that there is no migration in serial phases. } \XZ{I think we did not assume that there is no migration in serial phases.It is based on the logic that high parallelization means high possible potential improvements. Dividing by (R*|T|) means the average contention times per thread in a second.}

Here,  $M_i$ is the possible migration times of  thread $i$, $R$ is total running time (seconds) of the program, and $P$ is the parallel phase percentage. Overall, this formula computes the average migration times per thread per second at first, and then multiplies the value with the parallel phase percentage. The multiplication is introduced because applications with a small percentage of parallel phase should have a lower performance impact that can be caused by thread migration. 

a%ll threads in the program,  |T| is the number of total threads, $c_t$ is the lock contention times for thread $t$, and $R$ is total running seconds of the program.

Similarly, \NP{} predicts the possible migration times, instead of detecting the real migration times. \NP{} utilizes the total number of lock contentions, condition waits, and barrier waits as the estimation of possible migration times, which could be higher than the real migration times. To correct these numbers, \NP{} intercepts the above-mentioned synchronizations via a preloading mechanism, where lock contention can be identified by checking the status of the lock. After that, \NP{} further utilizes the Equation~\ref{equation:tms} to filter out serious issues that require user attention. 

The parallel phase percentage indicates the necessity of performing the optimization. For instance, if the parallel phase percentage is only 1\%, then we could at most improve the performance by 1\%, which is not worthy of the fixes.   In order to produce a similar score even when the number of threads is significantly different, the score is further divided by the number of threads. 
%Based on our evaluation, this parameter makes two platforms with different number of threads have very similar results. 

When an application has a large number of thread migrations,  \NP{} suggests users utilize thread binding to reduce remote accesses. As shown in Table~\ref{tab:numa_issues}, the thread migration may degrade the performance of an application (i.e., \texttt{fluidanimate}) by 418\%. This shows the importance to eliminate thread migration for such applications.  However, some applications in PARSEC (as not shown in Table~\ref{tab:numa_issues}) have very marginal performance improvement with thread binding. 

\subsubsection{Detecting Load Imbalance}
\label{sec:loadimbalance}

Load imbalance is another factor that could significantly affect the performance of the NUMA architecture. That is,  some hardware nodes may have more memory accesses than other nodes that  could lead to node imbalance and interconnect congestion. \NP{} detects load imbalance among different types of threads, which is omitted by existing NUMA-profilers.  


\NP{} records the number of memory accesses of every thread and then summarizes them together for the same type of threads. In particular, \NP{} focuses on accesses of heap objects and globals, which could cause remote accesses (and therefore performance issues). Threads with the same thread function are considered to be the same type of threads. If every type of threads has an imbalanced number of accesses, then \NP{} further confirms whether the number of threads for each type is proportional to the number of accesses from the particular type. If not, then the application has a load imbalance issue. 


\NP{} further predicts an optimal thread assignment based on the number of memory accesses. 
%A balance assignment is to balance  memory accesses from each type of threads. 
For instance, if the number of memory accesses on two type of threads has a one-to-two portion, then \NP{} will suggest to assign threads in one-to-two portion. Section~\ref{sec:casestudies} further evaluates \NP{}'s suggested assignment, where \NP{} significantly outperforms another work~\cite{SyncPerf}. Note that it is important to consider memory accesses from the libraries. Otherwise, \NP{} may suggest a thread assignment that cannot achieve good performance. 
%,   outperforms the state-of-the-art, %Experimental results show that \NP{}'s suggested assignment outperforms the state-of-the-art, achieving good performance for two evaluated applications with load imbalance issues. 


\begin{comment}
We also propose two mechanisms to solve load imbalance issues according thread based memory access pattern.First of all, \NP{} recorded how many memory access requests produced in each thread, and threads are classified into different groups based on the type of tasks executed on threads. If there is only one single thread group in the whole application, no load balanced problems will be reported.Unless, \NP{} will recommend how many threads are needed in each group based on memory access overheads per group-$Opg_{NUMA}$. Basically the group with more memory overheads will get more threads, since more memory access overheads typically means more workloads and can be eliminated by more workers.

\begin{equation}
Opt_{NUMA} = Lmr + Rmr * RL  \label{Opt_NUMA}
\end{equation}

Equation\ref{Opt_NUMA} defines $Opt_{NUMA}$(memory access overheads per thread), in which Lmr represents the number of local memory requests, Rmr represents the number of remote memory requests and RL means the average remote access latency normalized by local access latency. 2 is assigned to RL in \NP{} by heuristic.And $Opg_{NUMA}$(memory access overheads per group) is computed simply by summing $Opt_{NUMA}$ for each thread in this group.

On the other head, \NP{} also detects how "close" each two threads are, and closest threads should be binded into a same node to reduce remote memory access. Since \NP{} assumed each thread running on a different virtual NUMA node and the first touch policy used in OS, the underlying physical page will be in the "virtual node A" if thread A first touched the virtual page. So that if there are tons amount of remote access between two virtual nodes, it will bring some performance benefits after we bind the two threads in a single real NUMA node, since huge amount of potential remote memory access are eliminated for sure.However, if a virtual node A contains similar remote access with any other vitual nodes, there will be no much difference no matter binding thread A with any other threads.Thread A is call balanced thread in this case, and it is called unbalanced thread if thread A is obviously much closer with some threads but far away with others.In an application, if most threads are unbalanced threads, it is highly possible to get some benefits by binding closet threads into a same NUMA node.On the other wise, if most threads are balanced threads, there is not too much thing users can do.
\todo{can add some pictures here}

How to identify balanced thread and unbalanced thread is little trivial.For a thread A, the mean deviation is computed for the distance between thread A and all other threads.If the mean deviation is small, thread A will be marked as a balanced thread since it has very similar distance with other threads.On the other hand, if the mean deviation is big enough, the thread will be marked as unbalanced but it is not 100 percent.For example, if thread A is marked as balanced thread and its average number of remote memory requests is significantly bigger than other threads , it will influence the mean deviation of others.To solve this problem, \NP{} did two rounds computation of mean deviation to mark all threads as balanced or unbalanced.In the first round, all threads are involved and only balanced threads are marked if it contains a low mean deviation.In the second round, the threads that has bean marked as balanced in the first round will be removed and the mean deviation is computed again.All the unmarked threads will be marked based on the mean deviation value from the second round computation.Besides, main thread is always not involved in the two rounds computation, since main thread is usually balanced and contains huge amount of remote memory request which could influence the the mean deviation of others.
\todo{can add some pictures here}

After all unbalanced threads are marked, \NP{} will try to bind closest threads into a same cluster.In this case, a simple but an effective algorithm is applied.We assume K is number of NUMA nodes and N is the number of cores in a single node, so the ideal number of threads in a single cluster is N.We can imagine all unbalanced threads are nodes, and they are connected with undirected edges with length as their distance.\NP{} treats each node as a center and draw a circle with a radius of N.If two circles intersect with each other, their centers are the most closest N node for each other and should be in a same cluster.In a ideal situation, this could formulate less K clusters, which is the optimal thread clustering solution.And balanced threads can be assigned to the formulated clusters randomly.However in most cases, more than K clusters could be produced, since the complicated relationships between threads.So after the clusters are formed, \NP{} treat the thread with the most remote memory requests as a center in each cluster and draw a circle with radius of $2N$.If two cycles are intersected and the clusters that the two center threads belong to can be merged, the two clusters will be merged into one single cluster.As long as the total number of threads inside one cluster is less or equal to N, it is acceptable. \NP{} will continue this process until less than K clusters are formed or no any two clusters can be merged together because they already contain a big number of threads.After this process finished, balanced threads can be assigned to any cluster randomly. 
\todo{adding some equations or pictures}


\end{comment}
