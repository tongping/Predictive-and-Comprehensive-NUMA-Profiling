\section{Implementation}

\NP{} leverages compiler-based instrumentation to insert a handle function call for each memory access instruction from applications. By the interception, \NP{} can simulate how the memory is accessed in both page and cache level, and collect the sharing pattern when the memory object is freed by users.If they are never freed explicitly, \NP{} can do the collection in the end of the whole application.If too many remote memory access happened in a memory object, it needs an attention. Besides, \NP{} also collected thread based information like how many thread migration happened, how many different thread groups, and how many local and remote memory access incurred in each thread.Based on these thread based information, \NP{} could tell users what kinds of NUMA imbalance issues the application contained and how to fix it.

\subsection{Compiler-Based Instrumentation} 
The instrumentation is achieved by LLVM in the \NP{}. Basicly, LLVM compiles programs into IR (intermediate representation) which is also called LLVM Assembly Code, and it also provides lots of libraries and tools to manipulate the IR, like insert, delete or replace assembly codes or function calls.
To intercept memory access instructions, we made a pass function which is loaded by LLVM and it traversed each files, functions and instructions to identify the target points and insert a function call. The function call is implemented by \NP{}, through which \NP{} could intercept each memory access operation from target applications with its address and access type(read or write).

It is worth to notice \NP{} only focused on memory access to heap objects, since heap objects have much higher chance to be shared for multiple threads, which leads potential NUMA issues. If a object is only accessed by one single thread, it will never have NUMA issues, unless threads migrated to another nodes and \NP{} could also detect this problem that is detailed described in the following parts.Besides, the pass function should be placed at the very end of compiling optimization phase of LLVM, since most memory access instructions will be optimized out after huge amounts of compiling optimization strategies applied by LLVM. Based on our experiments, the number of memory access instructions could be reduce by half or more.So if the order of the pass function is not set properly, huge amounts of duplicated instrumentation calls will be made which not only brings more overheads but also makes the profiling results not accurate.

% What are the challenges of that? 
% What type of issues that we usually have. I remembered that we used to have high overhead with compiler-based instrumentation.

% What particular issue for the version of instrumentation. 

\subsection{Detecting Remote Accesses Caused by Sharing}
In the beginning, there are some assumptions \NP{} made here that need to be clarified.To make \NP{} independent from any hardware, OS and scheduling algorithm, \NP{} assumed that each thread could be scheduled to any node randomly. So we considered each thread running on a single NUMA node and this could influence how \NP{} differentiated remote access and local access.For example, if thread-A tries to access a memory page which is first touched by thread-B, this access will be considered as a remote access without considering which nodes thread-A and thread-B are located on in the real world.Because of the first touch policy applied by OS, the memory page will be located on the node that the first touching thread located on.In the \NP{}, we also record the first touching thread id for each page.

Basically, \NP{} used $Rpm_{NUMA}$(remote access number per millisecond) to indicate how serious the NUMA issues are for each object memory.Main memory access usually happened when the cache is missed.If an memory request hit CPU cache, it will not go to main memory and will not bring any NUMA issues.However the CPU cache hit rate is determined by lots of factors like hardware implementation and data access pattern of applications.So \NP{} simply considered the CPU cache hit rate as fixed. Besides, \NP{} also considered how many cache invalidation happened into account caused by a write after read operation.The reason why we also involved cache invalidation number is that if a cache line contains false sharing or true sharing problems, it will bring huge massive main memory access that can be ignored by the fixed CPU cache hit rate that \NP{} applied by heuristics.Besides, cache level sharing and page level sharing issues need different solutions to fix, which is also a critical reason why \NP{} collect both page level and cache level sharing information.

\begin{equation}
Rpm_{NUMA} = (Rmr * CACHE\_MISS\_RATE + Rinvalid)/T \label{Rpm_NUMA}
\end{equation}

Equation\ref{Rpm_NUMA} defines how the remote access number per millisecond is computed, where Rmr represent the number of remote memory request in applications, Rinvalid means the number of remote cache invalidation caused by write after remote memory read operations and T means how many milliseconds the application is running.For example, if thread-A and thread-B both contains copies of a memory block from node-C in their CPU cache, after a write operation happened to the memory block, the cache lines in the CPU cores that thread-A and thread-B located on are both invalidated.And this cache invalidation is remote cache invalidation, since the memory copies in the cache lines are from remote memory readings.

The number of remote memory requests is collected in page level. First of all, \NP{} only record how many memory access happened in each virtual page, and if it exceeds a threshold, a more detailed information will be recorded.In the detailed information, \NP{} will record how many remote access happened in a block wise inside the detailed page.When an object memory is freed, a total remote access number is computed for the object by summing up the the remote access number in the covered blocks by the object.Besides, \NP{} also record whether this page is shared by multiple threads or by single thread.If the page is shared by multiple threads, what users can do is very limited, except make the object memory page interleaved over nodes.Unless,some regular access pattern could be found, like certain range of memory only being accessed by certain thread, so that a block wised interleaved or memory-thread collocation could give better performance improvements.

The remote invalidation number is collected in cache level.
Similar with what \NP{} did in the page level, memory writing number is recorded in cache level in the beginning.After it exceeds a threshold and also with massive reading operation happened, more detailed cache level information will be traced.This simple strategy could efficiently reduce both memory and performance overheads.In details, if a reading operation happened, \NP{} will record that this piece of memory is copied into the cache line of the demanding thread.And when a writing operation happened, all the copies in the related cache lines will be invalidated.We can imagine that if the memory is extensively shared among threads and also contain huge amounts of both reading and writing operation,the invalidation number could be super huge, which brings huge amounts of NUMA remote accesses. 

Besides, to provide more help information to users about what is root cause of the problems and how to fix them, \NP{} also detect and classify what kinds of problems a cache line contains, like true sharing, false sharing and replicability.To achieve this goal, \NP{} traced writing operation in word level, if a cache line is extensively updated by multiple threads.For each word, if it is updated by multiple threads, \NP{} will mark it as true sharing, unless, false sharing will be marked.For false sharing problems, users usually can use padding to make the issue relieved.But for true sharing, it could be very hard.Further more, \NP{} also traced how many continuous reading operations contained in each cache line.If a cache line contains massive continuous reading operations,users could optimize this piece of memory by duplicating it over NUMA nodes, so that the continuous reading operations will become local access, and the overheads used to make them consistent will be very small.

\NP{} can not avoid big overheads due to the use of instrumentation.The situation could be worse if an application contains very serious NUMA issues or even cache sharing issues, since the issues can be doubled in the \NP{}.To eliminate this problem, \NP{} also did some sampling inside to reduce overheads, but without influencing its effects.Specifically, \NP{} make sampling for page level information but not for cache level information mentioned above.The reason is that the volume of memory request is much bigger than writing, so not too much information will be lost by sampling and the huge volume of memory request is one main reason of the significant overheads.This also brings one of the advantages of instrumentation methods that users can customize their sampling as they want.

%False sharing/ true sharing. 
%Duplicatable detect.
%Cache Invalidation Number (instead of using latency) 

\subsection{Detecting Remote Accesses Caused by Potential Migration} 
Even developers made a lot of efforts to do memory distribution or solve cache line sharing problems, they could still find not too much improvements in performance.One of very crucial reasons is that threads migrated to another node, but memory stayed still. So no matter how big efforts developers made to optimize their code, remote access still can not be eliminated.Even worse, in most cases stack memories are only accessed by a single thread, which are initially located on the same node with that thread.But the local access will become remote access after the thread makes migration.That is one reason that applications can get huge performance improvements just by simply thread binding based on our extensive experiments.

Typically, it is highly possible that threads made migration if lock contention happened.Unless it is highly possible the thread will be rescheduled to the origin core.So \NP{} will only check whether a thread migrates to another node or not when a thread gets blocked because of lock waiting, and this also helps to reduce the overheads caused by frequently kernel function call to get current NUMA node id.To check whether a thread gets blocked or not, \NP{} also intercept all blocking lock function calls in standard C library, like pthread\_mutex\_lock and pthread\_barrier\_wait.Inside the \NP{}, it stimulated the behaviors of each blocking lock before acquired the real lock function,so that we could get the lock contention status before the thread goes blocked.

\subsection{Detecting Load Imbalance}
We also propose two mechanisms to solve load imbalance issues according thread based memory access pattern.First of all, \NP{} recorded how many memory access requests produced in each thread, and threads are classified into different groups based on the type of tasks executed on threads.If there is only one single thread group in the whole application, no load balanced problems will be reported.Unless, \NP{} will recommend how many threads are needed in each group based on memory access overheads per group-$Opg_{NUMA}$. Basically the group with more memory overheads will get more threads, since more memory access overheads typically means more workloads and can be eliminated by more workers.

\begin{equation}
Opt_{NUMA} = Lmr + Rmr * RL  \label{Opt_NUMA}
\end{equation}

Equation\ref{Opt_NUMA} defines $Opt_{NUMA}$(memory access overheads per thread), in which Lmr represents the number of local memory requests, Rmr represents the number of remote memory requests and RL means the average remote access latency normalized by local access latency. 2 is assigned to RL in \NP{} by heuristic.And $Opg_{NUMA}$(memory access overheads per group) is computed simply by summing $Opt_{NUMA}$ for each thread in this group.

On the other head, \NP{} also detects how "close" each two threads are, and closest threads should be binded into a same node to reduce remote memory access.Since \NP{} assumed each thread running on a different virtual NUMA node and the first touch policy used in OS, the underlying physical page will be in the "virtual node A" if thread A first touched the virtual page. So that if there are tons amount of remote access between two virtual nodes, it will bring some performance benefits after we bind the two threads in a single real NUMA node, since huge amount of potential remote memory access are eliminated for sure.However, if a virtual node A contains similar remote access with any other vitual nodes, there will be no much difference no matter binding thread A with any other threads.Thread A is call balanced thread in this case, and it is called unbalanced thread if thread A is obviously much closer with some threads but far away with others.In an application, if most threads are unbalanced threads, it is highly possible to get some benefits by binding closet threads into a same NUMA node.On the other wise, if most threads are balanced threads, there is not too much thing users can do.
\todo{can add some pictures here}

How to identify balanced thread and unbalanced thread is little trivial.For a thread A, the mean deviation is computed for the distance between thread A and all other threads.If the mean deviation is small, thread A will be marked as a balanced thread since it has very similar distance with other threads.On the other hand, if the mean deviation is big enough, the thread will be marked as unbalanced but it is not 100 percent.For example, if thread A is marked as balanced thread and its average number of remote memory requests is significantly bigger than other threads , it will influence the mean deviation of others.To solve this problem, \NP{} did two rounds computation of mean deviation to mark all threads as balanced or unbalanced.In the first round, all threads are involved and only balanced threads are marked if it contains a low mean deviation.In the second round, the threads that has bean marked as balanced in the first round will be removed and the mean deviation is computed again.All the unmarked threads will be marked based on the mean deviation value from the second round computation.Besides, main thread is always not involved in the two rounds computation, since main thread is usually balanced and contains huge amount of remote memory request which could influence the the mean deviation of others.
\todo{can add some pictures here}

After all unbalanced threads are marked, \NP{} will try to bind closest threads into a same cluster.In this case, a simple but an effective algorithm is applied.We assime K is number of NUMA nodes and N is the number of cores in a single node, so the ideal number of threads in a single cluster is N.We can imagine all unbalanced threads are nodes, and they are connected with undirected edges with length as their distance.\NP{} treats each node as a center and draw a circle with a radius of N.If two circles intersect with each other, their centers are the most closest N node for each other and should be in a same cluster.In a ideal situation, this could formulate less K clusters, which is the optimal thread clustering solution.And balanced threads can be assigned to the formulated clusters randomly.However in most cases, more than K clusters could be produced, since the complicated relationships between threads.So after the clusters are formed, \NP{} treat the thread with the most remote memory requests as a center in each cluster and draw a circle with radius of $2N$.If two cycles are intersected and the clusters that the two center threads belong to can be merged, the two clusters will be merged into one single cluster.As long as the total number of threads inside one cluster is less or equal to N, it is acceptable. \NP{} will continue this process until less than K clusters are formed or no any two clusters can be merged together because they already contain a big number of threads.After this process finished, balanced threads can be assigned to any cluster randomly. 
\todo{adding some equations or pictures}