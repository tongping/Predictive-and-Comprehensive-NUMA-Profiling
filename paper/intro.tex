\section{Introduction}
\label{sec:intro}

The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design for multi-core and multi-processor era. Compared to the Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller, where each node/processor can access its own memory controller concurrently. However, the NUMA architecture imposes multiple system challenges for parallel applications, such as remote accesses, interconnect congestion, and node imbalance~\cite{Blagodurov:2011:CNC:2002181.2002182}, as further described in Section~\ref{sec:numa}. That is, user programs are prone to different types of performance issues, which indicates the necessity of developing profiling tools to identify NUMA-related performance issues. 

Existing NUMA-related profiling tools could be largely classified into two types. One type is employing coarse-grained sampling~\cite{}, while the other type of tools are built on the fine-grained instrumentation. These tools have their own pros and cons. On the one hand, sampling-based tools are able to identify performance issues within the deployment environment, due to their lower overhead. On the other hand, fine-grained tools, such as Maco~\cite{}, MemPerf~\cite{}, and NUMAProf~\cite{}) are able to detect more performance issues, but with the compromise of the performance overhead. Due to this fact, fine-grained tools are more suitable for the development phase, which aims to detect as many performance issues as possible before the release of software. 

%\todo{Do existing work differentiate false sharing and true sharing? This is important since false sharing and true sharing will require different fixes, and most true-sharing can be very difficult to fix} 

These existing tools also share some \textbf{common and serious issues}. \textit{First, they have the portability issue, which can only identify performance issues on the current hardware}. 
%That is, they cannot predictively detect potential performance issues on a different hardware topology.  
In order to identify a remote access, they requires to collect the node of each thread and the node information of every access: if the physical memory is located in a node that is different from a node that the thread is running on, then a remote access is detected. \textit{Second, they have the comprehensiveness issue, which may miss important optimization opportunities}. Some of this fact are caused by their implementation. For instance, in order to reduce the overhead of identifying the thread location, some tools bind every thread to a specific node that allows to infer the thread node using a thread-private variables~\cite{XuNuma}, instead of employing a system call.  However, they may miss some issues due to their binding. A more serious issue is that existing tools only focuses one particular reason that will cause remote accesses, but omitting other reasons that may also introduce remote accesses. Also, they cannot detect NUMA issues caused by other reasons, such as node imbalance. As a result, they could only detect a small portion of performance issues, as shown in our evaluation. \textit{Third, existing tools could not report detailed reasons of performance issues, and cannot provide sufficient guideline for bug fixes}. This will leave significant burden for users in order to figure out a fix strategy. For instance, they cannot report whether remote accesses are caused by true or false sharing, which clearly requires different strategy since true sharing may not be able to fix easily.   


This paper proposes a novel tool, called \NP{}, which proposes different mechanisms to overcome these issues of existing tools. \NP{} is designed as an automatic tool that does not require human annotation or the change of the code. It also does not require new hardware, or the change of the underlying operating system. \NP{} aims to detect multiple types of NUMA-related issues in the development phase.
%when applications are fed with representative inputs.  

% compiler-based instrumentation
% Due to the portability issue
First, \NP{} proposes a predictive mechanism that could detect all potential NUMA-related performance issues, even without running on a NUMA architecture. 
This paper is based on a key observation that \textit{a thread can run on any physical node due to the scheduling}. Based on this observation, \NP{} only tracks the relationship between memory accesses and threads, instead of the node relationship, which greatly simplifies the detection difficulty: there is no need to know the node that a thread is running on; there is no need to track the physical node information of each page. Instead, \NP{} only requires to track sharing pattern of memory accesses. This observation also makes it possible to detect all potential issues within development phases, instead of paying unnecessary runtime overhead in deployment environment, as far as applications are fed with representative inputs. 


Second, \NP{} improves its detection effectiveness by extending its detection scope. As mentioned above, applications may suffer significant performance issues due to remote accesses, memory controller congestion, and interconnect congestion, but existing tools only detect partial issues related to remote accesses. Based on our understanding, memory controller congestion and interconnection congestion are also related with load imbalance. Therefore, \NP{} extends its detection scope to identify both remote accesses and load imbalance. For remote accesses, \NP{} also predicts potential cross-node thread migration since that will significantly affect the performance, by up to $5\times$ based on our evaluation. Cross-node migration basically turns all previous local accesses into remote accesses. A migrated thread is forced to reload its data that is already existing in its previous cache, and access its stack and pages remotely. Further, it will also disrupt memory allocations to create non-local allocations, since most existing allocators will not return freed objects to its original node. \NP{} detects potential load imbalances among threads, which instruments all memory accesses in order to predict the load imbalance correctly. 

Last but not least, \NP{} aims to provide sufficient information that helps bug fixes, overcoming one common issue of existing tools. For remote accesses caused by sharing, \NP{} will differentiate false sharing from true sharing, cache-level sharing from page-level sharing, which will have different fix strategies. For instance, cache-level false sharing could be fixed easily by using the padding, but that cannot be done for page-level false sharing or true sharing. If remote accesses can be caused by thread migration, \NP{} will further suggest to use thread binding for some applications. For load imbalance, \NP{} will further suggest the thread clustering for particular threads, and suggest the adjusting number for each type of threads in order to achieve the load balance.   

%We discovered that applications with a high number of system calls or synchronizations will have a high potential to have thread migration. That is due to the reason that a thread will be placed into a waiting queue, upon synchronizations or system calls, then it could be migrated to a different node by the OS scheduler.


We have performed extensive evaluations to verify the effectiveness of \NP{}. \NP{} detected many more performance issues than existing tools, including both fine-grained and coarse-grained tools. After fixing these issues, some application could achieve the performance improvement up to %$X\times$. 
\NP{} further provides sufficient information for bug fixes, which is exemplified by few case studies. 
%We further evaluates \NP{}'s performance and memory overhead. \NP{} introduces around $X\times$ performance overhead and 

Overall, \NP{} has the following contributions. 

\begin{itemize}
    \item \NP{} designs the first tool that could predictively detect NUMA-related performance issues without relying on a specific architecture. 
    \item \NP{} proposes multiple mechanisms that can detect more NUMA-related performance issues caused by thread migration and load imbalance, which are currently omitted by existing tools. 
    \item \NP{} provides helpful information to assist bug fixes, which is lacked in existing tools. 
    \item We have performed extensive evaluations to confirm \NP{}'s effectiveness and overhead.  
\end{itemize}

\begin{comment}
 Existing work is typically bound to a specific architecture. 
 
 In addition, they could only identify one type of issue that a NUMA issue inside the application. However, they cannot explain whether a NUMA issue is caused by the allocator. 
 
 We observe that NUMA performance issue is similar to cache contention, but can occur in both cache and page granularity. Therefore, we could utilize the same framework to identify both cache contention (false/true sharing) and page-level false sharing issue, instead of only on the page granularity. 
 
 We also observe that existing profilers only work on a particular architecture. 
 
 Third, existing profilers could not identify the issues caused by the allocator, which makes them cannot explain performance issue of an allocator. 
 
 
 \NP{} will have the following differences. 
 \begin{itemize}
 \item \NP{} does not rely on a specific hardware, which could identify issues in any potential hardware. 
 \item \NP{} could identify the issue caused by the memory allocator, such as passive and active false sharing of objects. 
 \item \NP{} could even identify the metadata issues of a memory allocator. 
 \item \NP{} could identify both cache and page sharing, where both of them have a significant performance impact on the application. 
 \end{itemize}

 


Our work is to derive some potential problems of existing applications on the given NUMA hardware, then provides an insight to users how to solve the problem by fixing existing applications. And our work will utilize the on-line analysis. 

% see the paper: Toward the efficient use of multiple explicitly managed memory subsystems.
In this paper, we use emulator-based profiling to analyze actual program executions when programs are executed, this setup allows us to associates the cache misses with the different memory objects of the executed application. 
This paper shares the similar target as our paper. 

~\cite{Bolosky:1989:SBE:74850.74854} is their original work that talks about the page management. 


We observe that the number of cache loading is the most important metrics.
In order to evaluate the performance impact, we propose to utilize the number of cache invalidations to evaluate the seriousness of NUMA issues. A more intuitive metrics is the cache line loading operations. However, it is impossible to do this very easily. Thus, we proposes to utilize the number of cache invalidation as the metrics. 


However, cache invalidations can be affected by two factors: cache-line based sharing, and size-related invalidations. 
Size-related invalidations indicate that one core's cache is not sufficient to cover the memory footprint of the thread. Therefore, some cache lines will be evicted, which will cause cause invalidation. However, it is not easy to do this, which will be omitted by the \NP{}. 

Instead, \NP{} focuses cache invalidations caused by sharing, either true or false sharing. Similarly, page-level sharing will end up with the same issue, due to the existence of cache. If the data is held in the cache line and the data is valid, then there will be no remote accesses even if the actual physical page is located remotely. 

Therefore, we propose \NP{} to be a profiling tool that could identify both page and cache-level sharing. Also, \NP{} is based on the observation that applications may have performance issue even if it does not in the current architecture. 

However, the challenge is to evaluate the number of cache invalidations correctly and efficiently. Predator also utilizes the similar mechanism, but cannot compute the number correctly. Then we will give an example about this. 

\NP{} instead will compute the number differently. It will has the number of copies for all threads for each thread, and then the detailed information for each word (which helps to solve the issue). For each write operation, we will increment the number of copies correspondingly, instead of just one.

However, there will be some efficiency issue on the performance and the memory. We can't have the information for each cache line, which will at least impose around $2\times$ memory overhead. Also, we could easily cause significant performance issue if we have to traverse the cache line for all memory writes. 
 
\todo{Should we traverse the whole cache line? Or we will use the word-based structure? Then when there is invalidation, then we will clean the per-line flag. } Then we will guarantee that every access will only invoke $O(1)$ operations. 


To reduce the memory overhead, we will utilize the number of cache misses and the number of threads as a metrics. If a cache line is only accessed by one thread, then it is possibly only accessed by a thread, then it should not cause the performance issue. 
Also, if the number of writes on a cache line is small, it will never be the performance issue for most cases. 

However, we may need to save the number of cache misses for each callsite together. Otherwise, some objects will be skipped. 


\NP{} is different from existing work that it also profiles the performance issue caused by allocators.  

Major difference:

1. MACPO focus on the code with identified bottleneck, not the whole program.
2. MACPO only works on arrays, unions, structures, not all variables. 
3. They care about reuse-distance, and XXX. We may only care about the number of remote accesses
4. They use offline analysis, not sure whether that is one of reason that cause substantial overhead sinceit will have tons of IO operations. We don't know whether the analysis online will have a better performance
5. MACPO only output the reuse distance, remote access number, cycles per access (because they utilize the cycle-based cache simulation), access strides (for prefetching). They have output like Figure2. 

\end{comment}
 





