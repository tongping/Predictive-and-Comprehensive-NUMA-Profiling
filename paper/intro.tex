\section{Introduction}
\label{sec:intro}

The Non-Uniform Memory Access (NUMA) architecture is a scalable hardware design in the multi-core and multi-processor era. Compared to the Uniform Memory Access (UMA) architecture, the NUMA architecture avoids the bottleneck of using one memory controller by allowing each node/processor to concurrently access its own memory controller. However, the NUMA architecture imposes multiple system challenges for parallel applications, such as remote accesses, interconnect congestion, and node imbalance~\cite{Blagodurov:2011:CNC:2002181.2002182}, as further described in Section~\ref{sec:numa}. User programs could easily suffer from performance issues caused by those challenges, necessitating the development of profiling tools to identify NUMA-related performance issues. 

General-purpose profiling tools, such as \texttt{gprof}~\cite{DBLP:conf/sigplan/GrahamKM82}, \texttt{perf}~\cite{perf}, or \texttt{Coz}~\cite{Coz}, are not suitable for identifying NUMA-related performance issues~\cite{XuNuma,NumaPerf} because they are agnostic to the architecture difference. 
Existing NUMA-related profiling tools can be largely classified into two types. One type employs coarse-grained sampling to identify performance issues in the deployment environment~\cite{XuNuma}, while the other type builds on fine-grained instrumentation that could detect more performance issues but with a higher overhead. 
%Due to this fact, fine-grained tools are more suitable for the development phase, which aims to detect as many performance issues as possible before the release of software. 

%\todo{Do existing work differentiate false sharing and true sharing? This is important since false sharing and true sharing will require different fixes, and most true-sharing can be very difficult to fix} 

However, these existing tools have the following \textbf{common issues}. \textit{First, they have the portability issue. They can only identify performance issues on the current NUMA hardware}. 
%That is, they cannot predictively detect potential performance issues on a different hardware topology.  
Typically, they require threads to be bound to cores and rely on  node information to detect a remote access: a remote access happens when the physical memory a thread accesses is located in a remote node that is different from the one that the thread is running on. \textit{Second, they suffer from the effectiveness issue, leading to the miss of many optimization opportunities}. They only focus on one type of remote accesses, while omitting other reasons (e.g., thread migration) that could also incur remote accesses. Further, they completely omit load imbalance, another source of performance degradation. As a result, they could only detect a small portion of performance issues, as shown in our evaluation. \textit{Third, existing tools could not provide sufficient guidelines for bug fixes}. This will leave significant burden for users in order to figure out a fix strategy. For instance, they cannot report whether remote accesses are caused by true or false sharing, which clearly requires different fix strategy since true sharing may not be able to fix easily.   


This paper proposes a novel tool---\NP{}---that overcomes these issues of existing tools. \NP{} is designed as an automatic tool that does not require human annotation or the change of the code. It also does not require new hardware, or the change of the underlying operating system. \NP{} aims to detect all NUMA-related issues in the development phase, if applications are exercised with representative inputs. In this way, there is no need for paying additional runtime overhead in deployment phase. We further describe \NP{}'s distinctive features in the following. 

First, \NP{} proposes a predictive mechanism that could detect performance issues for any NUMA architecture, even without running on a NUMA architecture. It is based on a key observation that \textit{a thread can run on any physical node due to scheduling}. Based on this observation, \NP{} tracks the relationship between memory accesses and threads, instead of their physical nodes. It greatly simplifies the detection problem: there is no need to know the node on which a thread is running; there is no need to track the physical node information of each page. Instead, \NP{} tracks only sharing pattern of memory accesses. The observation also makes it possible to detect all potential issues within development phases, instead of paying unnecessary runtime overhead in a deployment environment, assuming applications are fed with representative inputs. 


Second, \NP{} will identify more issues that may affect performance in NUMA architecture. As mentioned above, applications may suffer significant performance issues due to remote accesses, memory controller congestion, and interconnect congestion, but existing tools detect only partial issues related to remote accesses. Based on our understanding, memory controller congestion and interconnection congestion are also related with load imbalance. Therefore, \NP{} extends its detection scope to identify both remote accesses and load imbalance. For remote accesses, \NP{} also predicts potential cross-node thread migration because it will significantly affect the performance, by up to $5\times$ based on our evaluation. Cross-node migration basically turns all previous local accesses into remote accesses. A migrated thread is forced to reload its data that already exists in its previous cache, and access its stack and pages remotely. Further, it will also disrupt memory allocations to create non-local allocations, since most existing allocators will not return freed objects to its original node. \NP{} detects potential load imbalances among threads. It instruments all memory accesses in order to predict the load imbalance correctly. 

Last but not least, \NP{} aims to provide sufficient information that helps bug fixes, overcoming one common issue of existing tools. For remote accesses caused by sharing, \NP{} will differentiate false sharing, true sharing, cache-level sharing, and page-level sharing. These sharing patterns will have different fix strategies. For instance, padding can easily fix cache-level false sharing but is impractical for page-level false sharing or true sharing. If remote accesses are caused by thread migration, \NP{} will suggest thread binding for some applications. For load imbalance, \NP{} will suggest thread clustering for particular threads and the adjusting number for each type of threads in order to balance workloads.   

%We discovered that applications with a high number of system calls or synchronizations will have a high potential to have thread migration. That is due to the reason that a thread will be placed into a waiting queue, upon synchronizations or system calls, then it could be migrated to a different node by the OS scheduler.


We have performed extensive evaluations to verify the effectiveness of \NP{}. \NP{} detected many more performance issues than existing tools, including both fine-grained and coarse-grained tools. \HG{what is the benchmark?} After fixing these issues, these applications could achieve up to ??$\times$ performance improvement. 
\NP{} also provides sufficient information for bug fixes, which is exemplified by few case studies. 
%We further evaluates \NP{}'s performance and memory overhead. \NP{} introduces around $X\times$ performance overhead and 

Overall, \NP{} makes the following contributions. 

\begin{itemize}
    \item \NP{} is the first tool that could predictively detect NUMA-related performance issues without relying on a specific NUMA architecture. 
    \item \NP{} proposes multiple mechanisms to detect a comprehensive set of NUMA-related performance issues, including those caused by thread migration and load imbalance but omitted by existing tools. 
    \item \NP{} provides helpful information to assist bug fixes, which is lacked in existing tools. 
    \item We have performed extensive evaluations to confirm \NP{}'s effectiveness and overhead.  
\end{itemize}




\begin{comment}
 Existing work is typically bound to a specific architecture. 
 
 In addition, they could only identify one type of issue that a NUMA issue inside the application. However, they cannot explain whether a NUMA issue is caused by the allocator. 
 
 We observe that NUMA performance issue is similar to cache contention, but can occur in both cache and page granularity. Therefore, we could utilize the same framework to identify both cache contention (false/true sharing) and page-level false sharing issue, instead of only on the page granularity. 
 
 We also observe that existing profilers only work on a particular architecture. 
 
 Third, existing profilers could not identify the issues caused by the allocator, which makes them cannot explain performance issue of an allocator. 
 
 
 \NP{} will have the following differences. 
 \begin{itemize}
 \item \NP{} does not rely on a specific hardware, which could identify issues in any potential hardware. 
 \item \NP{} could identify the issue caused by the memory allocator, such as passive and active false sharing of objects. 
 \item \NP{} could even identify the metadata issues of a memory allocator. 
 \item \NP{} could identify both cache and page sharing, where both of them have a significant performance impact on the application. 
 \end{itemize}

 


Our work is to derive some potential problems of existing applications on the given NUMA hardware, then provides an insight to users how to solve the problem by fixing existing applications. And our work will utilize the on-line analysis. 

% see the paper: Toward the efficient use of multiple explicitly managed memory subsystems.
In this paper, we use emulator-based profiling to analyze actual program executions when programs are executed, this setup allows us to associates the cache misses with the different memory objects of the executed application. 
This paper shares the similar target as our paper. 

~\cite{Bolosky:1989:SBE:74850.74854} is their original work that talks about the page management. 


We observe that the number of cache loading is the most important metrics.
In order to evaluate the performance impact, we propose to utilize the number of cache invalidations to evaluate the seriousness of NUMA issues. A more intuitive metrics is the cache line loading operations. However, it is impossible to do this very easily. Thus, we proposes to utilize the number of cache invalidation as the metrics. 


However, cache invalidations can be affected by two factors: cache-line based sharing, and size-related invalidations. 
Size-related invalidations indicate that one core's cache is not sufficient to cover the memory footprint of the thread. Therefore, some cache lines will be evicted, which will cause cause invalidation. However, it is not easy to do this, which will be omitted by the \NP{}. 

Instead, \NP{} focuses cache invalidations caused by sharing, either true or false sharing. Similarly, page-level sharing will end up with the same issue, due to the existence of cache. If the data is held in the cache line and the data is valid, then there will be no remote accesses even if the actual physical page is located remotely. 

Therefore, we propose \NP{} to be a profiling tool that could identify both page and cache-level sharing. Also, \NP{} is based on the observation that applications may have performance issue even if it does not in the current architecture. 

However, the challenge is to evaluate the number of cache invalidations correctly and efficiently. Predator also utilizes the similar mechanism, but cannot compute the number correctly. Then we will give an example about this. 

\NP{} instead will compute the number differently. It will has the number of copies for all threads for each thread, and then the detailed information for each word (which helps to solve the issue). For each write operation, we will increment the number of copies correspondingly, instead of just one.

However, there will be some efficiency issue on the performance and the memory. We can't have the information for each cache line, which will at least impose around $2\times$ memory overhead. Also, we could easily cause significant performance issue if we have to traverse the cache line for all memory writes. 
 
\todo{Should we traverse the whole cache line? Or we will use the word-based structure? Then when there is invalidation, then we will clean the per-line flag. } Then we will guarantee that every access will only invoke $O(1)$ operations. 


To reduce the memory overhead, we will utilize the number of cache misses and the number of threads as a metrics. If a cache line is only accessed by one thread, then it is possibly only accessed by a thread, then it should not cause the performance issue. 
Also, if the number of writes on a cache line is small, it will never be the performance issue for most cases. 

However, we may need to save the number of cache misses for each callsite together. Otherwise, some objects will be skipped. 


\NP{} is different from existing work that it also profiles the performance issue caused by allocators.  

Major difference:

1. MACPO focus on the code with identified bottleneck, not the whole program.
2. MACPO only works on arrays, unions, structures, not all variables. 
3. They care about reuse-distance, and XXX. We may only care about the number of remote accesses
4. They use offline analysis, not sure whether that is one of reason that cause substantial overhead sinceit will have tons of IO operations. We don't know whether the analysis online will have a better performance
5. MACPO only output the reuse distance, remote access number, cycles per access (because they utilize the cycle-based cache simulation), access strides (for prefetching). They have output like Figure2. 

\end{comment}
 





